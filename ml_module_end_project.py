# -*- coding: utf-8 -*-
"""ml_module_end_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4rYXejXPesadaRIJcSwDhdRfBF5H_qY
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Reading the dataset
cars = pd.read_csv('/content/CarPrice_Assignment.csv')

cars_df = cars.copy()

cars_df.head()

cars_df.shape

# Summary of the dataset
cars_df.info()

# Description of the dataset
cars_df.describe().T

# Checking for null values
cars_df.isnull().sum()

# Checking for duplicates
cars_df.loc[cars.duplicated()]

"""No duplicates found"""

# Dropping duplicated records (if any)
cars_df = cars_df.drop_duplicates()

# Dropping car_ID attribute
cars_df.drop('car_ID', axis=1, inplace=True)

# Converting symbolin to categorical
cars_df['symboling'] = cars_df['symboling'].astype('object')
cars_df.info()

# Splitting company name from CarName column
CompanyName = cars_df['CarName'].apply(lambda x : x.split(' ')[0])

CompanyName

# Adding Company name to DataFrame
cars_df.insert(3, 'CompanyName', CompanyName)

# Dropping CarName from the DataFrame
cars_df.drop(['CarName'], axis=1, inplace=True)

cars_df.head()

# Unique Company Names
cars_df['CompanyName'].unique()

# Renaming the mis-spelled Car Brands
cars_df['CompanyName'].replace('maxda', 'mazda', inplace=True)
cars_df['CompanyName'].replace('Nissan', 'nissan', inplace=True)
cars_df['CompanyName'].replace('porcshce', 'porsche', inplace=True)
cars_df['CompanyName'].replace('toyouta', 'toyota', inplace=True)
cars_df['CompanyName'].replace(['vokswagen', 'vw'], 'volkswagen', inplace=True)

cars_df['CompanyName'].unique()

# Unique fuel type
cars_df['fuelsystem'].unique()

# Renaming mis-spelled fuel type
cars_df['fuelsystem'].replace('mfi', 'mpfi')

# Visalise the Data
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.title('Car Price Distribution')

sns.distplot(cars_df.price, color='y')

plt.subplot(1, 2, 2)
plt.title('Car Price Spread')
sns.boxplot(y=cars_df.price, color='y')

plt.show()

# Checking car price distribution
cars_df.price.describe(percentiles = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90,1])

"""#Insights:
1. From the graphs, it seems that data is right skewed, which means most car prices in the dataset are below 17,493
2. Around 80% of the car price is below 17,493
"""

# Visualisation of numerical data by features
fig, axes = plt.subplots(5, 3, figsize=(12, 10))
fig.suptitle('Scatter plot visualisation of numerical data by features')

sns.scatterplot(ax=axes[0, 0], data=cars_df, x='carlength', y='price')
sns.scatterplot(ax=axes[0, 1], data=cars_df, x='carwidth', y='price')
sns.scatterplot(ax=axes[0, 2], data=cars_df, x='carheight', y='price')

sns.scatterplot(ax=axes[1, 0], data=cars_df, x='curbweight', y='price')
sns.scatterplot(ax=axes[1, 1], data=cars_df, x='enginesize', y='price')
sns.scatterplot(ax=axes[1, 2], data=cars_df, x='boreratio', y='price')

sns.scatterplot(ax=axes[2, 0], data=cars_df, x='stroke', y='price')
sns.scatterplot(ax=axes[2, 1], data=cars_df, x='compressionratio', y='price')
sns.scatterplot(ax=axes[2, 2], data=cars_df, x='horsepower', y='price')

sns.scatterplot(ax=axes[3, 0], data=cars_df, x='wheelbase', y='price')
sns.scatterplot(ax=axes[3, 1], data=cars_df, x='citympg', y='price')
sns.scatterplot(ax=axes[3, 2], data=cars_df, x='highwaympg', y='price')

sns.scatterplot(ax=axes[4, 0], data=cars_df, x='peakrpm', y='price')

plt.tight_layout()
plt.show()

# Numerical features
cars_numeric = cars_df.select_dtypes(exclude='object')

# Correlation matrix
cor = cars_numeric.corr()

cor

# Plotting correlations on a heatmap
plt.figure(figsize=(16, 8))
sns.heatmap(cor, cmap='rainbow', annot=True)

"""#Insights:
1. Features 'carwidth', 'carlength', 'curbweight', 'enginesize', 'boreratio', 'horsepower' and 'wheelbase' have positive correlation with price
2. 'carheight', 'stroke', 'compressionratio', 'peakrpm' doesn't have any correlation with price.
3. 'citympg' and 'highwaympg' have negative correlation with price



"""

# Removing 'carheight', 'stroke', 'compressionratio'
cars_df = cars_df.drop(columns=['carheight', 'stroke', 'compressionratio'])

cars_df.columns

# Categorical features
cars_categorical = cars_df.select_dtypes(include=['object'])

cars_categorical.columns

# Encoding categorical variables
dummy = pd.get_dummies(cars_categorical, drop_first=True)

# Dropping categorical variables
cars_df = cars_df.drop(columns=cars_categorical)

# Adding Encoded variables to DataFrame
cars_df = pd.concat([cars_df, dummy], axis=1)

cars_df.head()

# Separate features and target

X = cars_df.drop('price', axis=1)

y = cars_df['price']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""##Model implementation"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

# Define models
models = {
    'Linear Regression' : LinearRegression(),
    'Decision Tree' : DecisionTreeRegressor(random_state=42),
    'Random Forest' : RandomForestRegressor(random_state=42),
    'Gradient Boosting' : GradientBoostingRegressor(random_state=42),
    'Support Vector Regressor' : SVR()
}

# Train and evaluate models
results = {}
for name, model in models.items():
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  results[name] = y_pred

"""##Model Evaluation"""

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Evaluate models
evaluation = {}
for name, y_pred in results.items():
  evaluation[name] = {
      'R2' : r2_score(y_test, y_pred),
      'MSE' : mean_squared_error(y_test, y_pred),
      'MAE' : mean_absolute_error(y_test, y_pred)
  }

# Display results
for model, metrics in evaluation.items():
  print(f'Model : {model}')
  for metric, value in metrics.items():
    print(f'{metric} : {value}')
  print()

"""###Best Performing Model : Random Forest Regressor

* R²: 0.9554 (Highest, indicating strong predictive
power)
* MSE : 3,514,725 (Lowest, showing minimal error in predictions)
* MAE : 1,305 (Lowest, reflecting the least average deviation from actual prices)

###Feature Importance Analysis
"""

import matplotlib.pyplot as plt

# Use the Random Forest Regressor
feature_importances = models['Random Forest'].feature_importances_

# Visualize feature importance
features = X.columns
plt.figure(figsize=(10, 6))
plt.barh(features, feature_importances)
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance')
plt.show()

from sklearn.feature_selection import RFE, SelectKBest, mutual_info_regression

# Recursive Feature Elimiination (RFE) with Random Forest Regressor
rfe_selector_rf = RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=5)
rfe_selector_rf.fit(X, y)

# Selected features using RFE
selected_features_rfe = X.columns[rfe_selector_rf.support_]
print('Top 5 features using RFE with Random Forest:')
print(list(selected_features_rfe))

# SelectKBest with mutual_info_regression
kbest_selector_rf = SelectKBest(score_func=mutual_info_regression, k=5)
kbest_selector_rf.fit(X, y)

# Selected features using SelectKBest
selected_features_kbest = X.columns[kbest_selector_rf.get_support()]
print('\nTop 5 features using SelectKBest with mutual_info_regression:')
print(list(selected_features_kbest))

from sklearn.feature_selection import f_regression

# Splitting categorical and numerical features
categorical_cols = X.select_dtypes(include=['object', 'bool']).columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

X_categorical = X[categorical_cols]
X_numerical = X[numerical_cols]

# Mutual Information Regression for Categorical Features
mi_selector = SelectKBest(score_func=mutual_info_regression, k=5)
X_mi = mi_selector.fit_transform(X_categorical, y)
mi_selected_features = X_categorical.columns[mi_selector.get_support()]

# ANOVA F-test for Numerical Features
f_selector = SelectKBest(score_func=f_regression, k=5)
X_f = f_selector.fit_transform(X_numerical, y)
f_selected_features = X_numerical.columns[f_selector.get_support()]

# Display selected features
print('Mutual Information selected for Categorical features:')
print(list(mi_selected_features))
print('\nANOVA (F-test) selected for Numerical features:')
print(list((f_selected_features)))

"""* Recursive Feature Elimination (RFE) with Random Forest Regressor to capture model-based importance
* Mutual Information Regression for categorical features to assess non-linear dependencies. It was specifically applied to categorical features to understand their influence on car prices.
* ANOVA F-test for numerical features to determine statistical variance-based importance. It was applied to numerical features to quantify their contribution to the target variable.

###Hyperparameter Tuning
"""

import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Objective function for optimization
def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 100, 1000)
    max_depth = trial.suggest_int('max_depth', 5, 50)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)
    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])

    rf = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        random_state=42
    )
    rf.fit(X_train, y_train)
    preds = rf.predict(X_test)

    r2 = r2_score(y_test, preds)
    mse = mean_squared_error(y_test, preds)
    mae = mean_absolute_error(y_test, preds)

    return r2

# Hyperparameter optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Best Parameters and Evaluation
print("Best Parameters:", study.best_params)
print("Best R² Score:", study.best_value)

# Evaluate with Best Model
best_model = RandomForestRegressor(**study.best_params, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print(f"Optimized R²: {r2_score(y_test, y_pred):.4f}")
print(f"Optimized MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"Optimized MAE: {mean_absolute_error(y_test, y_pred):.2f}")

"""For hyperparameter tuning, Optuna was used due to its efficient, automated optimization approach, leveraging a Tree-structured Parzen Estimator (TPE) for faster convergence compared to traditional grid or random search methods.

###Performance of the model has increased
* R²: 0.9566
* MSE: 3422998.40
* MAE: 1287.83

> R² Score (Higher is Better):

* Initial Model: 0.9554

* Optimized Model: 0.9566 (Slight improvement)

> Mean Squared Error (MSE) (Lower is Better):

* Initial Model: 3,514,725

* Optimized Model: 3422998.40 (Reduced error)

> Mean Absolute Error (MAE) (Lower is Better):

* Initial Model: 1,305

* Optimized Model: 1287.83 (Less average deviation)


"""

